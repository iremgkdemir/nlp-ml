{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: red\"> **3.Ön İşleme (Pre-processing):** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('C:\\\\NLP_INTRODUCTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', download_dir='C:\\\\NLP_INTRODUCTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#c9486f\"> Tokenizasyon </span>\n",
    "\n",
    "- Tokenizasyon, metni daha küçük parçalara, yani tokenlara (belirteçlere) ayırma işlemidir.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#a95ec5 \">  NLTK'de Bulunan Tokenizer Türleri:  </span>\n",
    "\n",
    "- **word_tokenize():** verilen bir metin dizgesini (string) kelimelere ayırır ve bu kelimeleri bir liste olarak döndürür. Her kelime veya noktalama işareti bir token (belirteç) olarak değerlendirilir. Metni kelime ve belirli noktalama işaretlerine göre tokenlara böler.\n",
    "\n",
    "- **wordpunct_tokenize():** bir metni hem kelimelere hem de noktalama işaretlerine göre ayıran bir fonksiyondur. Kelimeler arasındaki boşlukları kullanarak kelimeleri ayırır ve aynı zamanda noktalama işaretlerini de ayrı tokenlar olarak değerlendirir.Metni kelimeler ve tüm noktalama işaretlerine göre daha ayrıntılı şekilde böler.\n",
    "\n",
    "- **sent_tokenize()** bir metni cümlelere ayıran bir fonksiyondur. Metindeki cümle sonlandırma noktalarını (nokta, ünlem işareti, soru işareti vb.) kullanarak metni cümlelere böler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "text = \"Machine Learning is a Subset of Artificial intelligence, and it does with learning from data without being explicitly programmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'Learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Subset',\n",
       " 'of',\n",
       " 'Artificial',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'does',\n",
       " 'with',\n",
       " 'learning',\n",
       " 'from',\n",
       " 'data',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize fonksiyonunu kullanarak token içindeki cümleyi kelimelere ayırır. \n",
    "# Sonuç, kelimelerden oluşan bir liste olan tokens değişkenine atanır.\n",
    "ml_tokens = word_tokenize(text)\n",
    "ml_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'Learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Subset',\n",
       " 'of',\n",
       " 'Artificial',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'does',\n",
       " 'with',\n",
       " 'learning',\n",
       " 'from',\n",
       " 'data',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_tokens_punct = wordpunct_tokenize(text)\n",
    "ml_tokens_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine Learning is a Subset of Artificial intelligence, and it does with learning from data without being explicitly programmed']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_tokens_sent = sent_tokenize(text)\n",
    "ml_tokens_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#a95ec5 \"> Bigrams, Trigrams ve Ngrams: </span>\n",
    "\n",
    "- **Bigramlar:** Bir cümledeki ardışık iki kelimelik tokenlar.\n",
    "- **Trigramlar:** Bir cümledeki ardışık üç kelimelik tokenlar.\n",
    "- **Ngramlar:** Bir cümledeki herhangi bir sayıda ardışık kelimelik tokenlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'Learning'),\n",
       " ('Learning', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'Subset'),\n",
       " ('Subset', 'of'),\n",
       " ('of', 'Artificial'),\n",
       " ('Artificial', 'intelligence'),\n",
       " ('intelligence', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'it'),\n",
       " ('it', 'does'),\n",
       " ('does', 'with'),\n",
       " ('with', 'learning'),\n",
       " ('learning', 'from'),\n",
       " ('from', 'data'),\n",
       " ('data', 'without'),\n",
       " ('without', 'being'),\n",
       " ('being', 'explicitly'),\n",
       " ('explicitly', 'programmed')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams, trigrams\n",
    "list(nltk.bigrams(ml_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'Learning', 'is'),\n",
       " ('Learning', 'is', 'a'),\n",
       " ('is', 'a', 'Subset'),\n",
       " ('a', 'Subset', 'of'),\n",
       " ('Subset', 'of', 'Artificial'),\n",
       " ('of', 'Artificial', 'intelligence'),\n",
       " ('Artificial', 'intelligence', ','),\n",
       " ('intelligence', ',', 'and'),\n",
       " (',', 'and', 'it'),\n",
       " ('and', 'it', 'does'),\n",
       " ('it', 'does', 'with'),\n",
       " ('does', 'with', 'learning'),\n",
       " ('with', 'learning', 'from'),\n",
       " ('learning', 'from', 'data'),\n",
       " ('from', 'data', 'without'),\n",
       " ('data', 'without', 'being'),\n",
       " ('without', 'being', 'explicitly'),\n",
       " ('being', 'explicitly', 'programmed')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(ml_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#c9486f\"> POS Tagging </span>\n",
    "\n",
    "- Tokenizasyon işlemi yapıldıktan sonra, elde edilen her bir token (kelime), POS Tagging işlemi ile dil bilgisi açısından bir etiketle sınıflandırılır. \n",
    "- Örneğin, bir kelimenin isim mi, fiil mi, zamir mi olduğu belirlenir. \n",
    "- POS Tagging, tokenizasyon olmadan yapılamaz çünkü her kelimenin hangi sözcük türü olduğunu belirlemek için önce kelimelerin ayrı ayrı belirlenmesi (tokenize edilmesi) gerekir.\n",
    "\n",
    "\n",
    "- ##### örnek: \"I like to read books\" \n",
    "    - \"I\" (PRP) -> Zamir\n",
    "    - \"like\" (VBP) -> Fiil (geniş zaman)\n",
    "    - \"to\" (TO) -> Bağlaç\n",
    "    - \"read\" (VB) -> Fiil (mastarı)\n",
    "    - \"books\" (NNS) -> İsim (çoğul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yaygın POS Etiketi Kısaltmaları:\n",
    "| Kısaltma | Anlamı | Türkçe Karşılığı | Örnekler |\n",
    "|----------|--------|------------------|----------|\n",
    "| **NN**   | Noun, singular or mass | Tekil isim | cat, dog, car |\n",
    "| **NNS**  | Noun, plural | Çoğul isim | cats, dogs, cars |\n",
    "| **NNP**  | Proper noun, singular | Özel isim, tekil | John, London, Python |\n",
    "| **NNPS** | Proper noun, plural | Özel isim, çoğul | Americas, Sundays |\n",
    "| **VB**   | Verb, base form | Fiil, temel form | run, go, eat |\n",
    "| **VBD**  | Verb, past tense | Fiil, geçmiş zaman | ran, went, ate |\n",
    "| **VBG**  | Verb, gerund or present participle | Fiil, -ing eki almış hali | running, going, eating |\n",
    "| **VBN**  | Verb, past participle | Fiil, geçmiş zaman sıfat-fiil | gone, eaten, written |\n",
    "| **VBP**  | Verb, non-3rd person singular present | Fiil, üçüncü tekil şahıs dışındaki geniş zaman formu | run, go, eat |\n",
    "| **VBZ**  | Verb, 3rd person singular present | Fiil, üçüncü tekil şahıs geniş zaman formu | runs, goes, eats |\n",
    "| **JJ**   | Adjective | Sıfat | big, green, fast |\n",
    "| **JJR**  | Adjective, comparative | Karşılaştırmalı sıfat | bigger, greener, faster |\n",
    "| **JJS**  | Adjective, superlative | Üstünlük belirten sıfat | biggest, greenest, fastest |\n",
    "| **RB**   | Adverb | Zarf | quickly, softly, well |\n",
    "| **RBR**  | Adverb, comparative | Karşılaştırmalı zarf | faster, sooner, better |\n",
    "| **RBS**  | Adverb, superlative | Üstünlük belirten zarf | fastest, soonest, best |\n",
    "| **PRP**  | Personal pronoun | Kişi zamiri | I, you, he, she |\n",
    "| **PRP$** | Possessive pronoun | İyelik zamiri | my, your, his, her |\n",
    "| **IN**   | Preposition or subordinating conjunction | Edat veya bağlaç | in, on, at, which |\n",
    "| **DT**   | Determiner | Belirteç | a, an, the, this |\n",
    "| **CC**   | Coordinating conjunction | Bağlaç | and, but, or |\n",
    "| **CD**   | Cardinal number | Asıl sayı | one, two, 100 |\n",
    "| **EX**   | Existential there | Varlık bildiren \"there\" | There is, There are |\n",
    "| **FW**   | Foreign word | Yabancı dilde kelime | bon, ciao |\n",
    "| **MD**   | Modal | Yardımcı fiil | can, should, will |\n",
    "| **POS**  | Possessive ending | İyelik eki | 's |\n",
    "| **TO**   | \"to\" as preposition | İlgeç olarak \"to\" | to go, to read |\n",
    "| **UH**   | Interjection | Ünlem | oh, wow, hey |\n",
    "| **WP**   | Wh-pronoun | Soru zamiri | who, what, which |\n",
    "| **WP$**  | Possessive wh-pronoun | İyelik soru zamiri | whose |\n",
    "| **WRB**  | Wh-adverb | Soru zarfı | how, where, when |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger',  download_dir='C:\\\\NLP_INTRODUCTION')\n",
    "nltk.download('averaged_perceptron_tagger_eng',  download_dir='C:\\\\NLP_INTRODUCTION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine => NN\n",
      "Learning => VBG\n",
      "is => VBZ\n",
      "a => DT\n",
      "Subset => NN\n",
      "of => IN\n",
      "Artificial => JJ\n",
      "intelligence => NN\n",
      ", => ,\n",
      "and => CC\n",
      "it => PRP\n",
      "does => VBZ\n",
      "with => IN\n",
      "learning => VBG\n",
      "from => IN\n",
      "data => NNS\n",
      "without => IN\n",
      "being => VBG\n",
      "explicitly => RB\n",
      "programmed => NN\n"
     ]
    }
   ],
   "source": [
    "for token in ml_tokens:\n",
    "    # tokenleştirilmiş kelimeler listesini NLTK'nin pos_tag fonksiyonunu kullanarak POS (Part of Speech) etiketleri ile birlikte bir liste olarak döndürür. \n",
    "    # Bu liste, her kelime ve onun dil bilgisi etiketini (isim, fiil, sıfat vb.) içeren çiftlerden (tuples) oluşur.\n",
    "    pos_tags = nltk.pos_tag(word_tokenize(token))\n",
    "    for word, tag in pos_tags:\n",
    "        print(f'{word} => {tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('İrem', 'NN')]\n",
      "[('eats', 'NNS')]\n",
      "[('an', 'DT')]\n",
      "[('apple', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"İrem eats an apple\"\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8  \"> \n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "#### **Düzenli İfade (Regex) Nedir?** \n",
    "\n",
    "</span>\n",
    "\n",
    "- **Tanım:** Düzenli ifadeler (regex), metin içinde desen eşleştirme (pattern matching) yapmak için kullanılan bir dildir. Belirli karakter dizilerini tanımlamak, bulmak veya değiştirmek için kullanılır.\n",
    "- **Kullanım Alanı:** Düzenli ifadeler, metin içinde belirli karakter dizilerini yakalamak, kelimeleri, sayıları, tarihleri ve diğer belirli formatları tespit etmek için kullanılır.\n",
    "\n",
    "##### **düzenli ifade üç ana bileşenden oluşur:**\n",
    "\n",
    "1.`(?u)\\W+`\n",
    "\n",
    "- **(?u):** Bu ifade, düzenli ifadenin Unicode uyumlu modda çalışmasını sağlar. Unicode mod, farklı dillerdeki karakterlerin düzgün bir şekilde tanınmasını sağlar. Python 3 ve üstünde bu mod varsayılan olarak açıktır, dolayısıyla belirtilmesine gerek olmayabilir.\n",
    "\n",
    "- **\\W:** Bu ifade, alfanümerik olmayan herhangi bir karakteri temsil eder. Yani harfler, rakamlar ve alt çizgi (_) hariç tüm karakterlerdir. Örneğin boşluklar, noktalama işaretleri, özel karakterler (örneğin, !, @, #, .) bu gruba girer.\n",
    "\n",
    "    - **+:** Bu işaret, bir veya daha fazla tekrarlayan karakteri yakalamak için kullanılır. Yani \\W+, bir veya daha fazla alfanümerik olmayan karakteri yakalar.\n",
    "\n",
    "    - **Örnek:** Cümledeki boşluklar veya noktalama işaretleri bu ifade ile yakalanır.\n",
    "\n",
    "2.`\\$\\[\\d\\.]+`\n",
    "\n",
    "- **\\$:** Bu, dolar işaretini ($) yakalar. \\ işareti, dolar işaretinin özel bir karakter olarak değil, kelime olarak algılanmasını sağlar.\n",
    "\n",
    "- **\\[ ve \\]:** Köşeli parantezler [ ve ], bir karakter sınıfı tanımlar. Yani köşeli parantezler içindeki karakterlerden herhangi birini yakalar.\n",
    "\n",
    "- **\\d:** Bu, herhangi bir rakamı (0-9) temsil eder.\n",
    "\n",
    "-  Normalde, nokta herhangi bir karakteri temsil eder, ancak `\\.` şeklinde yazıldığında yalnızca gerçek nokta karakterini temsil eder.\n",
    "\n",
    "- **+:** Bir veya daha fazla tekrarlayan karakteri yakalar.\n",
    "\n",
    "    - **Özetle:** \\$\\[\\d\\.]+ ifadesi, $ işaretiyle başlayan ve ardından sayı veya nokta içeren karakterleri yakalar. Bu, para birimi gibi formatları yakalamak için kullanılabilir.\n",
    "\n",
    "    - **Örnek:** \"$3.99\", \"$15\" gibi değerler bu düzenli ifadeyle yakalanır.\n",
    "\n",
    "3. `\\S+`\n",
    "\n",
    "- **\\S:** Bu ifade, boşluk olmayan herhangi bir karakteri temsil eder. Yani harfler, rakamlar, noktalama işaretleri gibi tüm karakterleri yakalar.\n",
    "\n",
    "    - **Özetle:** \\S+, boşluk olmayan herhangi bir karakter dizisini yakalar. Bu genellikle bir kelime veya bir grup karakter olarak değerlendirilir.\n",
    "    \n",
    "    - **Örnek:** \"hello\", \"world\", \"Python3!\" gibi boşluk içermeyen kelime grupları bu düzenli ifadeyle yakalanır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RegexpTokenizer, metni özel olarak belirlenmiş kurallara göre tokenlara ayırmak için kullanılır. \n",
    "- Diğer basit tokenizasyon yöntemlerinin aksine, burada düzenli ifadelerle daha karmaşık kurallar tanımlanabilir. \n",
    "    - Örneğin, $ işaretini veya sayıları ayrı bir token olarak yakalamak istiyorsak, RegexpTokenizer kullanabiliriz.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('İrem', 'NN')]\n",
      "[(' ', 'NN')]\n",
      "[('eats', 'NNS')]\n",
      "[(' ', 'NN')]\n",
      "[('an', 'DT')]\n",
      "[(' ', 'NN')]\n",
      "[('apple', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# SyntaxWarning: invalid escape sequence hatası olmamsı için raw string(r) kullan\n",
    "# verilen düzenli ifadeye göre metni parçalara ayır\n",
    "# Düzenli ifade, tokenizasyonun nasıl yapılacağını belirler.\n",
    "reg_tokenizer = RegexpTokenizer(r'(?u)\\W+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = reg_tokenizer.tokenize(sent)\n",
    "\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:#a95ec5\">\n",
    " \n",
    " -------------------------------------------------------------------------------------------\n",
    " ### **Stop Words (Durma Kelimeleri)** \n",
    " </span>\n",
    "\n",
    "- Stop words, bir dilde yaygın olarak kullanılan kelimeler kümesidir. Bu kelimeler, metin analizi sırasında genellikle göz ardı edilir çünkü çok sık kullanıldıkları için bilgi açısından pek bir değer taşımazlar.\n",
    "- Stop words, bir dilde yaygın olarak kullanılan kelimeler kümesidir. Bu kelimeler, metin analizi sırasında genellikle göz ardı edilir çünkü çok sık kullanıldıkları için bilgi açısından pek bir değer taşımazlar.\n",
    "- İngilizce'de yaygın durma kelimeleri \"a\", \"the\", \"is\", \"are\" gibi kelimelerdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords',  download_dir='C:\\\\NLP_INTRODUCTION')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "- Aşağıdaki satır, Python'da list comprehension adı verilen bir tekniği kullanarak bir listeyi filtrelemektedir.\n",
    "\n",
    "- List comprehension, Python'da bir listeyi çok daha kısa ve anlaşılır bir şekilde oluşturmanın bir yoludur. Genellikle döngüler ve koşullar ile birlikte kullanılır.\n",
    "    - Temel Yapısı:\n",
    "    - `[expression for item in iterable if condition]`\n",
    "\n",
    "    -her bir item üzerinde bir döngü çalıştırır ve condition (şart) doğru olduğunda expression ifadesini değerlendirir ve sonuçları bir liste olarak döndürür.\n",
    "\n",
    "- Bu kodun çalıştırılması sonucunda, ml_tokens listesindeki kelimelerden stop_words listesinde olmayanlar filtrelenerek filtered_data listesine eklenir.\n",
    "\n",
    "- Başka bir deyişle, durma kelimeleri çıkarılmış bir liste elde edilir.\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'Learning',\n",
       " 'Subset',\n",
       " 'Artificial',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'learning',\n",
       " 'data',\n",
       " 'without',\n",
       " 'explicitly',\n",
       " 'programmed']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = [w for w in ml_tokens if not w in stop_words]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#c9486f\"> \n",
    "\n",
    "-------------------------------------------------------------------\n",
    "# Kök Bulma\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8\"> \n",
    "\n",
    "\n",
    "### **1. Stemming**\n",
    "</span>\n",
    "\n",
    "Stemming, bir kelimenin kökünü (stem) veya kök formunu elde etmek amacıyla kelimenin sonundaki ekleri (inflections) ve diğer parçaları kaldırma işlemidir. Amaç, kelimenin temel formunu bulmaktır. Stemming işlemi genellikle kurallara dayalı olarak çalışır ve ortaya çıkan kök kelime her zaman gerçek bir kelime olmayabilir.\n",
    "\n",
    "\n",
    "###  NLTK'deki Stemming Türleri:\n",
    "\n",
    "- **Porter Stemmer:** Bu, en yaygın kullanılan ve İngilizce için tasarlanmış bir stemmer türüdür. Kök bulma işlemi sırasında daha hafif kurallar kullanır.\n",
    "    - Porter Stemmer, kelimelerden son ekleri kaldırarak daha kısa kökler elde eder. Ancak, bazen bu kökler gerçek İngilizce kelimeler olmayabilir (örneğin, genor).\n",
    "\n",
    "\n",
    "- **Lancaster Stemmer:** Daha agresif bir stemmer'dır. Kelimeleri daha fazla kısaltır ve kökleri genellikle Porter Stemmer'a göre daha kısa olur.\n",
    "    - Snowball Stemmer, bazı kelimeleri Porter Stemmer'dan farklı şekilde köklerine indirger. Örneğin, generate kelimesi generat olarak kısaltılırken, generous kelimesi kökünde kalır.\n",
    "\n",
    "\n",
    "- **Snowball Stemmer:** Porter Stemmer'ın geliştirilmiş bir versiyonudur ve çeşitli dillerde kullanılabilir.\n",
    "    - Lancaster Stemmer, kelimeleri çok kısa köklere indirger. Örneğin, \"generate\" ve \"generation\" gibi kelimeler aynı \"gen\" köküne indirgenir.\n",
    "    \n",
    "\n",
    "- **Regex Stemmer:** Düzenli ifadeler kullanarak kelimeleri köklerine indirger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous => gener\n",
      "generate => gener\n",
      "genorously => genor\n",
      "generation => gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "words = ['generous', 'generate','genorously', 'generation']\n",
    "for word in words:\n",
    "    print(f'{word} => {porter.stem(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous => generous\n",
      "generate => generat\n",
      "genorously => genor\n",
      "generation => generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "porter = SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(f'{word} => {porter.stem(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> gen\n",
      "generate --> gen\n",
      "genorously --> gen\n",
      "generation --> gen\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lan = LancasterStemmer()\n",
    "\n",
    "for word in words:\n",
    "  print(f\"{word} --> {lan.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> generou\n",
      "generate --> generate\n",
      "genorously --> genorously\n",
      "generation --> generation\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# ing, s ve able ile biten kelimeler kısaltılmaktadır. \n",
    "# Ancak minimum uzunluk olarak 4 karakter belirlenmiştir\n",
    "reg = RegexpStemmer('ing|s$|ables$',min=4)\n",
    "\n",
    "for word in words:\n",
    "  print(f\"{word} --> {reg.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8\"> \n",
    "\n",
    "-------------------------------------------------------------------\n",
    "### **2.Lemmatization**\n",
    "</span>\n",
    "\n",
    "- Lemmatization, kelimeleri sözlükteki kök formlarına (lemma) indirgeyen bir süreçtir.\n",
    "- Bu işlem sırasında kelimenin dil bilgisi (Part of Speech, POS) ve bağlamı dikkate alınır. \n",
    "    -  Örneğin, \"running\" kelimesi, fiil olarak kullanıldığında \"run\" olarak lemmatize edilir.\"ate\" kelimesi \"eat\" olarak lemmatize edilir.\n",
    "- Lemmatization, kelimenin geçerli bir sözlük kelimesi olmasını sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet',  download_dir='C:\\\\NLP_INTRODUCTION')\n",
    "nltk.download('omw-1.4',  download_dir='C:\\\\NLP_INTRODUCTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous => generous\n",
      "generate => generate\n",
      "genorously => genorously\n",
      "generation => generation\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word} => {lemma.lemmatize(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#c9486f\"> \n",
    "\n",
    "-------------------------------------------------------------------\n",
    "# Metinleri Daha Anlamlı Hale Getirmek ve Doğru Bilgi Çıkarımı\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8\"> \n",
    "\n",
    "\n",
    "### **1.Named Entity Recognition (NER)**\n",
    "</span>\n",
    "\n",
    "- Metin içinde yer alan belirli varlıkları (entity) tanımlama ve sınıflandırma işlemidir. Bu varlıklar genellikle kişi adları, yer adları, organizasyon isimleri, tarih veya parasal değerler gibi kategorilere ayrılır.\n",
    "\n",
    "- Sürecinin ilk adımıdır. Büyük metin yığınlarından anlamlı bilgiler çıkarmak için kullanılır.\n",
    "\n",
    "- NER algoritmaları, metin içindeki özel isimleri bulur ve bunları önceden tanımlanmış kategorilere ayırır. \n",
    "    - Örneğin, \"John\", \"Google\", \"New York\" gibi kelimeleri tanıyıp, sırasıyla \"Kişi\", \"Organizasyon\", \"Yer\" kategorilerine yerleştirir.\n",
    "\n",
    "- **ne_chunk:** Bu fonksiyon, metindeki kelimeleri ve kelime gruplarını adlandırılmış varlıklar (entities) olarak tanımlamak için kullanılır. NER (Named Entity Recognition) işlemini gerçekleştirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\NLP_INTRODUCTION...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker',  download_dir='C:\\\\NLP_INTRODUCTION')\n",
    "nltk.download('words',  download_dir='C:\\\\NLP_INTRODUCTION')\n",
    "nltk.download('maxent_ne_chunker_tab',  download_dir='C:\\\\NLP_INTRODUCTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE Apple\n",
      "GPE American\n",
      "GPE California\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "text = \"Apple is an American company based out of California\"\n",
    "\n",
    "# Metni tokenize ediyoruz ve POS etiketleme işlemi yapıyoruz\n",
    "tokenized_text = word_tokenize(text)\n",
    "pos_tagged_text = pos_tag(tokenized_text)\n",
    "\n",
    "# POS etiketli metin üzerinde ne_chunk ile adlandırılmış varlıkları buluyoruz\n",
    "for chunk in ne_chunk(pos_tagged_text):\n",
    "    # Eğer varlık bir nesne ise, etiketini ve içeriğini yazdır\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "\n",
    "        # chunk.label(): Varlığın türünü yazdırır.\n",
    "        # .join(c[0] for c in chunk): \"Chunk\" içindeki kelimeleri birleştirerek adlandırılmış varlıkları bir bütün olarak yazdırır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE Narendra\n",
      "GPE Modi\n",
      "GPE India\n"
     ]
    }
   ],
   "source": [
    "text = \"Narendra Modi is the Prime Minister of India\"\n",
    "for w in nltk.word_tokenize(text):\n",
    "  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(w))):\n",
    "     if hasattr(chunk, 'label'):\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8\"> \n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "### **2. Word Sense Disambiguation (WSD)**\n",
    "</span>\n",
    "\n",
    "- Bir kelimenin anlamının bağlama göre belirlenmesi işlemidir. Aynı kelimenin farklı anlamları olabilir ve WSD, bu anlamlardan hangisinin doğru olduğunu belirler.\n",
    "\n",
    "    - Örnek: \"bark\" kelimesi hem \"ağaç kabuğu\" hem de \"köpek havlaması\" anlamlarına gelebilir. Bu kelimenin hangi anlamda kullanıldığını belirlemek için cümledeki diğer kelimeler ve bağlam analiz edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#a95ec5\"> \n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "### **Lesk Algoritması**\n",
    "</span>\n",
    "\n",
    "- Lesk Algoritması, 1986 yılında Michael E. Lesk tarafından tanıtılan klasik bir Word Sense Disambiguation (WSD) algoritmasıdır. Algoritma, kelimelerin anlamlarının, içinde bulundukları cümledeki diğer kelimelerle benzer anlamlara sahip olacağı varsayımına dayanır.\n",
    "\n",
    "- lesk algoritması, bir kelimenin farklı anlamlarını içeren açıklamaları (sözlük tanımları gibi) karşılaştırır ve bu açıklamalardan hangisinin cümledeki diğer kelimelerle en çok örtüştüğünü belirleyerek doğru anlamı seçer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('jamming.n.01') deliberate radiation or reflection of electromagnetic energy for the purpose of disrupting enemy use of electronic devices or systems\n",
      "Synset('jam.v.05') get stuck and immobilized\n"
     ]
    }
   ],
   "source": [
    "a1= lesk(word_tokenize('The building has a device to jam the signal'),'jam')\n",
    "print(a1,a1.definition())\n",
    "a2 = lesk(word_tokenize('I am stuck in a traffic jam'),'jam')\n",
    "print(a2,a2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Burada lesk fonksiyonu, verilen cümledeki belirli bir kelimenin (bu örnekte \"jam\") anlamını belirlemek için kullanılır. Algoritma, kelimenin bağlamındaki diğer kelimelere bakarak en uygun anlamı seçer.\n",
    "\n",
    "    - ilk cümlede, \"bir sinyali engellemek amacıyla kasıtlı olarak elektromanyetik enerji yayılması\" anlamına geldiğini belirlemiştir.\n",
    "\n",
    "    - ikinci cümlede, \"trafikte sıkışmak ve hareketsiz kalmak\" anlamına geldiğini belirlemiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('current.a.01') occurring in or belonging to the present time\n",
      "Synset('stream.n.02') dominant course (suggestive of running water) of successive events or ideas\n"
     ]
    }
   ],
   "source": [
    "a1= lesk(word_tokenize('what is the current time ?'),'current')\n",
    "print(a1,a1.definition())\n",
    "a2 = lesk(word_tokenize('air current'),'current')\n",
    "print(a2,a2.definition())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
