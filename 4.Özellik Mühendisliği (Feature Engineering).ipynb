{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> **4.Özellik Mühendisliği (Feature Engineering)** </span>\n",
    "\n",
    "- Bu süreç, ham metnin sayısal bir forma dönüştürülmesini, yani metin temsilini içerir. \n",
    "\n",
    "- Makine öğrenimi ve derin öğrenme modelleri, metin verilerini sayısal vektörler halinde işleyebilir. Bu yüzden metin, sayılarla ifade edilen vektörlere dönüştürülmelidir. Metinleri vektör haline getirmek için kullanılan bazı yaygın yöntemler şunlardır:\n",
    "\n",
    "    - **One Hot Encoding:** Her kelimeyi, sadece bir yerde 1 olan ve diğer yerlerde 0 olan bir vektörle temsil eder.\n",
    "\n",
    "    - **Count Vectorizer:** Metindeki kelimelerin sayısını belirten bir matris oluşturur.\n",
    "\n",
    "    - **TF-IDF:** Bir kelimenin bir metin içindeki önemini hesaplar.\n",
    "\n",
    "    - **Word Embeddings:** Kelimelerin anlamlarını yakalayan ve benzer kelimeleri benzer sayısal vektörlerle ifade eden tekniklerdir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#c9486f\"> 1- one-hot Encoding </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['dog eats meat','man eats meat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'eats': 2, 'meat': 3, 'man': 4}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}   # empty dictionary\n",
    "count = 0   # initialize count to 0\n",
    "for doc in corpus:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            count = count + 1\n",
    "            vocab[word] = count  # her bir kelimeye bir sayı atıyoruz\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [0] * 4    # vector of 4 zeros\n",
    "l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1[0] = 1  # set the first element to 1\n",
    "l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(doc):  # bir metin belgesini (doc) girdisi olarak alır ve bu belgedeki her kelimeyi one-hot encoding yöntemiyle bir vektör haline getirir\n",
    "  one_hot = []  \n",
    "  for word in doc.split():\n",
    "    temp = [0]*len(vocab) # vocab uzunluğunda tüm elemanları sıfır olan bir liste (vektör) oluşturur, -1 Python'da indekslemenin 0'dan başlaması nedeniyle kullanılır\n",
    "    if word in vocab:\n",
    "      temp[vocab[word]-1] = 1   # kelimenin vocab içindeki indeksine 1 atar\n",
    "    one_hot.append(temp)    # one_hot listesine ekler\n",
    "  return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(\"dog eats meat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#a95ec5\"> **Scikit - Learn** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"dog bites meat\"\n",
    "doc2 = 'man eats meat'\n",
    "doc3 = 'dog bites man'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My overall data: ['dog', 'bites', 'meat', 'man', 'eats', 'meat', 'dog', 'bites', 'man']\n"
     ]
    }
   ],
   "source": [
    "corpus = [doc1.split(),doc2.split(),doc3.split()]   # split() metni boşluklara göre ayırır\n",
    "my_overall_data  = corpus[0] + corpus[1] + corpus[2]    # tüm belgeleri birleştirir\n",
    "\n",
    "print(f\"My overall data: {my_overall_data}\")\n",
    "\n",
    "#implement Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer Values are: [1 0 4 3 2 4 1 0 3]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "integer_data = le.fit_transform(my_overall_data) # listedek kelimeler LabelEncoder kullanılarak sayısal değerlere dönüştürülüyor. Her farklı kelime, benzersiz bir sayıya karşılık gelir.\n",
    "print(f\"Integer Values are: {integer_data}\")\n",
    "\n",
    "# \"dog\" kelimesi 1,\n",
    "# \"bites\" kelimesi 0,\n",
    "# \"meat\" kelimesi 4,\n",
    "# \"man\" kelimesi 3,\n",
    "# \"eats\" kelimesi 2 olarak kodlanmıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "one_hot_encoder.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder.transform([\"dog eats meat\".split()]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#a95ec5\"> One-hot encoding dezavantajları: </span>\n",
    "\n",
    "- One-hot vektörlerinin boyutu, kelime haznesinin (vocabulary) büyüklüğüne bağlıdır. Gerçek dünyadaki metin veri kümeleri genellikle çok büyük kelime haznelerine sahiptir, bu da çok büyük vektörlere yol açar.\n",
    "\n",
    "- One-hot encoding, genellikle veriyi seyrek (sparse) bir şekilde temsil eder. Bu, vektörlerde çoğunlukla 0 olduğu anlamına gelir ve bu da veriyi saklamayı ve işlemeyi zorlaştırır.\n",
    "\n",
    "- Seyrek vektörler, hem hafızada daha fazla yer kaplar hem de bu vektörlerle yapılan işlemler (hesaplamalar) daha fazla zaman alır. Bu, veriyi saklama, işleme ve öğrenmede verimsizlik yaratır.\n",
    "\n",
    "- One-hot encoding, kelimelerin sıralamasını dikkate almaz. Ancak, cümlelerdeki kelime sırası genellikle anlam açısından önemlidir. Bu yöntemde kelimeler sadece bağımsız olarak ele alınır.\n",
    "\n",
    "- Eğitim sırasında karşılaşılmayan kelimeler için one-hot encoding bir temsil sağlayamaz. Bu durum, yeni veya nadir kelimelerle karşılaşıldığında problem yaratır.\n",
    "\n",
    "- One-hot encoding, kelimenin geçtiği bağlamı dikkate almaz. Her kelime, diğer kelimelerden bağımsız olarak temsil edilir, bu da kelimelerin cümle içindeki anlamını tam olarak yakalayamaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#c9486f\"> 2- BoW </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of Words, metin verisinden anlamlı özellikler çıkarmak için kullanılan bir yöntemdir.\n",
    "- Bu yöntem, metni temsil ederken kelimelerin ne sıklıkta geçtiğine bakar. Örneğin, bir belgede belirli bir kelimenin kaç kez yer aldığını sayar.\n",
    "- \"Bag\" (torba) terimi, kelimelerin sırasının veya yapısının bu yöntemde dikkate alınmadığını belirtir. Yani, kelimelerin hangi sırada olduğuna bakılmadan sadece hangi kelimelerin bulunduğu önemlidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Harry Potter is an amazing movie!!\"\n",
    "doc2 = \"Harry Potter is the best movie!\"\n",
    "doc3 = \"Harry potter is so great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "doc1 = re.sub(r\"[^a-zA-Z0-9]\",\" \",doc1.lower()).split()\n",
    "doc2 = re.sub(r\"[^a-zA-Z0-9]\",\" \",doc2.lower()).split()\n",
    "doc3 = re.sub(r\"[^a-zA-Z0-9]\",\" \",doc3.lower()).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.sub(r\"[^a-zA-Z0-9]\", \" \", doc1.lower()):`\n",
    "\n",
    "- `doc1.lower()` Cümle önce küçük harflere dönüştürülüyor. Bu, \"Harry Potter\" gibi büyük harf içeren kelimeleri \"harry potter\" olarak değiştirir.\n",
    "- `r\"[^a-zA-Z0-9]\"` Bu düzenli ifade, harfler (a-z, A-Z) ve rakamlar (0-9) dışındaki tüm karakterleri bulur.\n",
    "- `\" \"` Bulunan karakterler bir boşlukla (\" \") değiştirilir. Bu, örneğin !! veya ! gibi işaretlerin kaldırılmasını sağlar.\n",
    "- `.split():` Son olarak, cümle kelimelere ayrılır (liste haline getirilir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['harry', 'potter', 'is', 'an', 'amazing', 'movie']\n",
      "['harry', 'potter', 'is', 'the', 'best', 'movie']\n",
      "['harry', 'potter', 'is', 'so', 'great']\n"
     ]
    }
   ],
   "source": [
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amazing',\n",
       " 'an',\n",
       " 'best',\n",
       " 'great',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'movie',\n",
       " 'potter',\n",
       " 'so',\n",
       " 'the'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = set(doc1+doc2+doc3)\n",
    "all_words   # üç cümlede yer alan tüm benzersiz kelimeleri alfabetik olmayan bir sırayla gösterir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOWrepresentation(all_words, doc):  # bir belgeyi ve tüm kelimeleri alır ve bu belgeyi bir BoW temsiline dönüştürür\n",
    "  bow = dict.fromkeys(all_words,0)  # all_words içindeki tüm kelimeleri anahtar olarak alır ve değerlerini 0 yapar\n",
    "  for word in doc:\n",
    "    bow[word] = doc.count(word)  # Eğer kelime o metinde geçiyorsa, o kelimenin kaç kez geçtiğini bow sözlüğüne yazar\n",
    "  return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 0,\n",
       " 'potter': 1,\n",
       " 'so': 0,\n",
       " 'best': 0,\n",
       " 'the': 0,\n",
       " 'movie': 1,\n",
       " 'is': 1,\n",
       " 'harry': 1,\n",
       " 'amazing': 1,\n",
       " 'an': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow1 = BOWrepresentation(all_words,doc1)\n",
    "bow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>great</th>\n",
       "      <th>potter</th>\n",
       "      <th>so</th>\n",
       "      <th>best</th>\n",
       "      <th>the</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>harry</th>\n",
       "      <th>amazing</th>\n",
       "      <th>an</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   great  potter  so  best  the  movie  is  harry  amazing  an\n",
       "0      0       1   0     0    0      1   1      1        1   1\n",
       "1      0       1   0     1    1      1   1      1        0   0\n",
       "2      1       1   1     0    0      0   1      1        0   0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow2 = BOWrepresentation(all_words,doc2)\n",
    "bow3 = BOWrepresentation(all_words,doc3)\n",
    "df_bow = pd.DataFrame([bow1,bow2,bow3])\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#a95ec5\"> **Scikit - Learn** </span>\n",
    "\n",
    "CountVectorizer sınıfı, metin verisini kelime frekanslarına dayalı olarak sayısal vektörlere dönüştürmek için kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(binary=True) # binary=True, kelimenin belgede geçip geçmediğini belirler\n",
    "doc1 = \"Harry Potter is an amazing movie!!\"\n",
    "doc2 = \"Harry Potter is the best movie!\"\n",
    "doc3 = \"Harry potter is so great\"\n",
    "cv_out = cv.fit_transform([doc1,doc2,doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_out.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>an</th>\n",
       "      <th>best</th>\n",
       "      <th>great</th>\n",
       "      <th>harry</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>potter</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amazing  an  best  great  harry  is  movie  potter  so  the\n",
       "0        1   1     0      0      1   1      1       1   0    0\n",
       "1        0   0     1      0      1   1      1       1   0    1\n",
       "2        0   0     0      1      1   1      0       1   1    0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_out.toarray(), columns= cv.get_feature_names_out())     # get_feature_names_out() metodu, kelime kelime kelime listesini döndürür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>an</th>\n",
       "      <th>best</th>\n",
       "      <th>great</th>\n",
       "      <th>harry</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>potter</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amazing  an  best  great  harry  is  movie  potter  so  the\n",
       "0        1   1     0      0      2   1      1       1   0    0\n",
       "1        0   0     1      0      1   1      1       1   0    1\n",
       "2        0   0     0      1      1   1      0       1   1    0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "doc1 = \"Harry Potter is an amazing movie , harry!!\"\n",
    "doc2 = \"Harry Potter is the best movie!\"\n",
    "doc3 = \"Harry potter is so great\"\n",
    "cv_out = cv.fit_transform([doc1,doc2,doc3])\n",
    "pd.DataFrame(cv_out.toarray(), columns= cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>amazing movie</th>\n",
       "      <th>amazing movie harry</th>\n",
       "      <th>an</th>\n",
       "      <th>an amazing</th>\n",
       "      <th>an amazing movie</th>\n",
       "      <th>best</th>\n",
       "      <th>best movie</th>\n",
       "      <th>great</th>\n",
       "      <th>harry</th>\n",
       "      <th>...</th>\n",
       "      <th>potter</th>\n",
       "      <th>potter is</th>\n",
       "      <th>potter is an</th>\n",
       "      <th>potter is so</th>\n",
       "      <th>potter is the</th>\n",
       "      <th>so</th>\n",
       "      <th>so great</th>\n",
       "      <th>the</th>\n",
       "      <th>the best</th>\n",
       "      <th>the best movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amazing  amazing movie  amazing movie harry  an  an amazing  \\\n",
       "0        1              1                    1   1           1   \n",
       "1        0              0                    0   0           0   \n",
       "2        0              0                    0   0           0   \n",
       "\n",
       "   an amazing movie  best  best movie  great  harry  ...  potter  potter is  \\\n",
       "0                 1     0           0      0      2  ...       1          1   \n",
       "1                 0     1           1      0      1  ...       1          1   \n",
       "2                 0     0           0      1      1  ...       1          1   \n",
       "\n",
       "   potter is an  potter is so  potter is the  so  so great  the  the best  \\\n",
       "0             1             0              0   0         0    0         0   \n",
       "1             0             0              1   0         0    1         1   \n",
       "2             0             1              0   1         1    0         0   \n",
       "\n",
       "   the best movie  \n",
       "0               0  \n",
       "1               1  \n",
       "2               0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,3)) # unigram, bigram, trigram\n",
    "doc1 = \"Harry Potter is an amazing movie , harry!!\"\n",
    "doc2 = \"Harry Potter is the best movie!\"\n",
    "doc3 = \"Harry potter is so great\"\n",
    "cv_out = cv.fit_transform([doc1,doc2,doc3])\n",
    "pd.DataFrame(cv_out.toarray(), columns= cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:#a95ec5\"> N-Gram Yönteminin Avantajları: </span>\n",
    "\n",
    "- N-gramlar, kelimelerin yalnızca sıklığını değil, aynı zamanda cümledeki sıralarını ve birbirleriyle olan bağlamlarını da yakalayabilir. Bu, metinlerdeki kelime kombinasyonlarının anlamını daha iyi temsil etmeye yardımcı olur.\n",
    "\n",
    "- Aynı n-gramlara sahip olan belgeler, birbirine daha yakın vektörlerle temsil edilir. Bu, metinlerin Euclidean (Öklidyen) uzayında birbirine yakın olmasını sağlar. Bu özellik, benzer belgeleri gruplama veya sınıflandırma görevlerinde faydalıdır.\n",
    "\n",
    "\n",
    "### <span style=\"color:#a95ec5\"> N-Gram Yönteminin Dezavantajları: </span>\n",
    "\n",
    "- N değeri arttıkça (yani unigramdan bigrama, trigramdan daha büyük n-gramlara geçtikçe), veri boyutu hızla büyür. Bu durum, vektörlerin büyük bir kısmının 0 olmasıyla sonuçlanır ve bu da seyrek matrisler (sparsity) oluşturur. Bu tür matrisler, veriyi saklama ve işleme açısından verimsiz olabilir.\n",
    "\n",
    "- N-gramlar, eğitim sırasında karşılaşılmayan (Out-of-Vocabulary, OOV) yeni kelime kombinasyonlarını işleyemez. Bu durum, n-gram modelinin yeni metinlerle karşılaştığında zorlanmasına neden olabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#c9486f\"> 3. TF-IDF (Term Frequency-Inverse Document Frequency) yöntemi  </span>\n",
    "\n",
    "- Daha önceki yaklaşımlarda (örneğin, CountVectorizer) her kelimenin metin içindeki önemi eşit kabul ediliyordu. Bu, sık kullanılan kelimelerin de nadir kelimelerle aynı değerde olmasına neden olur.\n",
    "- TF-IDF, bir kelimenin önemini metin içindeki diğer kelimelere ve korpustaki diğer metinlere göre ölçer. Bu, önemli kelimeleri öne çıkarmayı ve önemsiz olanları arka plana atmayı sağlar.\n",
    "\n",
    "- **Neden IDF Gereklidir?** Sık kullanılan kelimeler (örneğin \"is\", \"are\") yüksek terim frekansına sahip olur ve bu da bu kelimelerin metinde daha önemli gibi görünmesine neden olur.Bu tür sık kullanılan kelimelerin önemini azaltmak ve daha az kullanılan ama önemli olan kelimelere daha fazla ağırlık vermek için IDF kullanılır.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| ÖNEMLİ                                                                                     |\n",
    "|--------------------------------------------------------------------------------------------|\n",
    "| Bir kelimenin bir belgede en çok tekrarlandığında, ancak başka hiçbir belgede bulunmadığında skor yüksek olur | \n",
    "| Stopwords - skor düşük olur                                                                | \n",
    "| Bir cümlede tekrarlanan benzersiz kelimeler - skor yüksek olur                             | \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Amazing',\n",
       " 'Data',\n",
       " 'Deep',\n",
       " 'Science',\n",
       " 'a',\n",
       " 'an',\n",
       " 'career',\n",
       " 'current',\n",
       " 'in',\n",
       " 'is',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'of',\n",
       " 'subset',\n",
       " 'the',\n",
       " 'world'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sent = \"Data Science is an Amazing career in the current world\"\n",
    "second_sent = \"Deep learning is a subset of machine learning\"\n",
    "\n",
    "first_sent = first_sent.split(\" \")\n",
    "second_sent = second_sent.split(' ')\n",
    "vocab = set(first_sent).union(set(second_sent)) # Kümeler aynı kelimenin birden fazla geçmesini engeller ve sadece benzersiz kelimeleri saklar.\n",
    "                                                # .union() fonksiyonu, bu iki kümenin birleşimini alır, yani her iki cümledeki tüm benzersiz kelimeleri bir araya getirir.\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'career': 0, 'in': 0, 'world': 0, 'the': 0, 'a': 0, 'of': 0, 'is': 0, 'Data': 0, 'current': 0, 'learning': 0, 'subset': 0, 'Science': 0, 'Amazing': 0, 'machine': 0, 'Deep': 0, 'an': 0} \n",
      " {'career': 0, 'in': 0, 'world': 0, 'the': 0, 'a': 0, 'of': 0, 'is': 0, 'Data': 0, 'current': 0, 'learning': 0, 'subset': 0, 'Science': 0, 'Amazing': 0, 'machine': 0, 'Deep': 0, 'an': 0}\n"
     ]
    }
   ],
   "source": [
    "wordDict1 = dict.fromkeys(vocab,0)  # vocab içindeki tüm kelimeleri anahtar olarak alır ve değerlerini 0 yapar\n",
    "wordDict2 = dict.fromkeys(vocab,0)\n",
    "print(wordDict1, \"\\n\",wordDict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'Science', 'is', 'an', 'Amazing', 'career', 'in', 'the', 'current', 'world'] \n",
      " {'career': 1, 'in': 1, 'world': 1, 'the': 1, 'a': 0, 'of': 0, 'is': 1, 'Data': 1, 'current': 1, 'learning': 0, 'subset': 0, 'Science': 1, 'Amazing': 1, 'machine': 0, 'Deep': 0, 'an': 1} \n",
      "\n",
      "['Deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning'] \n",
      " {'career': 0, 'in': 0, 'world': 0, 'the': 0, 'a': 1, 'of': 1, 'is': 1, 'Data': 0, 'current': 0, 'learning': 2, 'subset': 1, 'Science': 0, 'Amazing': 0, 'machine': 1, 'Deep': 1, 'an': 0}\n"
     ]
    }
   ],
   "source": [
    "#FREKANS HESAPLAMA\n",
    "for word in first_sent:\n",
    "  wordDict1[word] += 1  # kelimenin vocab içindeki indeksine 1 ekler. Böylece, first_sent içinde kaç kez geçtiği sayılır.\n",
    "\n",
    "for word in second_sent:\n",
    "  wordDict2[word] += 1\n",
    "  \n",
    "print(first_sent, \"\\n\", wordDict1, \"\\n\")\n",
    "print(second_sent, \"\\n\",wordDict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career</th>\n",
       "      <th>in</th>\n",
       "      <th>world</th>\n",
       "      <th>the</th>\n",
       "      <th>a</th>\n",
       "      <th>of</th>\n",
       "      <th>is</th>\n",
       "      <th>Data</th>\n",
       "      <th>current</th>\n",
       "      <th>learning</th>\n",
       "      <th>subset</th>\n",
       "      <th>Science</th>\n",
       "      <th>Amazing</th>\n",
       "      <th>machine</th>\n",
       "      <th>Deep</th>\n",
       "      <th>an</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   career  in  world  the  a  of  is  Data  current  learning  subset  \\\n",
       "0       1   1      1    1  0   0   1     1        1         0       0   \n",
       "1       0   0      0    0  1   1   1     0        0         2       1   \n",
       "\n",
       "   Science  Amazing  machine  Deep  an  \n",
       "0        1        1        0     0   1  \n",
       "1        0        0        1     1   0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([wordDict1,wordDict2])   # dataFrame oluşturulur\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8\"> \n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "### **Terim Frekansını(TF) Hesaplama**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTF(wordDict, doc):\n",
    "  tfDict = {}\n",
    "  len_corpus = len(doc) #belgedeki toplam kelime sayısı\n",
    "\n",
    "  for word,count in wordDict.items():\n",
    "    tfDict[word] = count/len_corpus # kelimenin geçiş sayısı / toplam kelime sayısı\n",
    "  \n",
    "  return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = calculateTF(wordDict1,first_sent)\n",
    "tf2 = calculateTF(wordDict2,second_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = calculateTF(wordDict1,first_sent)\n",
    "tf2 = calculateTF(wordDict2,second_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career</th>\n",
       "      <th>in</th>\n",
       "      <th>world</th>\n",
       "      <th>the</th>\n",
       "      <th>a</th>\n",
       "      <th>of</th>\n",
       "      <th>is</th>\n",
       "      <th>Data</th>\n",
       "      <th>current</th>\n",
       "      <th>learning</th>\n",
       "      <th>subset</th>\n",
       "      <th>Science</th>\n",
       "      <th>Amazing</th>\n",
       "      <th>machine</th>\n",
       "      <th>Deep</th>\n",
       "      <th>an</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   career   in  world  the      a     of     is  Data  current  learning  \\\n",
       "0     0.1  0.1    0.1  0.1  0.000  0.000  0.100   0.1      0.1      0.00   \n",
       "1     0.0  0.0    0.0  0.0  0.125  0.125  0.125   0.0      0.0      0.25   \n",
       "\n",
       "   subset  Science  Amazing  machine   Deep   an  \n",
       "0   0.000      0.1      0.1    0.000  0.000  0.1  \n",
       "1   0.125      0.0      0.0    0.125  0.125  0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = pd.DataFrame([tf1,tf2])\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iremg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'career': 1, 'in': 1, 'world': 1, 'the': 1, 'a': 0, 'of': 0, 'is': 1, 'Data': 1, 'current': 1, 'learning': 0, 'subset': 0, 'Science': 1, 'Amazing': 1, 'machine': 0, 'Deep': 0, 'an': 1} \n",
      " ['career', 'world', 'Data', 'current', 'learning', 'subset', 'Science', 'Amazing', 'machine', 'Deep'] \n",
      "\n",
      "\n",
      "{'career': 0, 'in': 0, 'world': 0, 'the': 0, 'a': 1, 'of': 1, 'is': 1, 'Data': 0, 'current': 0, 'learning': 2, 'subset': 1, 'Science': 0, 'Amazing': 0, 'machine': 1, 'Deep': 1, 'an': 0} \n",
      " ['career', 'world', 'Data', 'current', 'learning', 'subset', 'Science', 'Amazing', 'machine', 'Deep']\n"
     ]
    }
   ],
   "source": [
    "# wordDict1 içindeki tüm kelimeleri küçük harf olarak alır ve bu kelimelerin İngilizce stopwords listesinde olup olmadığını kontrol eder. \n",
    "# Eğer kelime stopwords değilse, bu kelime yeni bir listeye eklenir.\n",
    "\n",
    "# list comprehension\n",
    "f1 = [word for word in wordDict1 if word.lower() not in stopwords.words('english') ]\n",
    "f2 = [word for word in wordDict2 if word.lower() not in stopwords.words('english') ]\n",
    "\n",
    "\n",
    "print(wordDict1, \"\\n\", f1 , \"\\n\\n\")\n",
    "print(wordDict2, \"\\n\", f2)                  # stopwords listesinde olmayan kelimeleri gösterir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8\"> \n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "### **IDF hesaplama**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TF-IDF (Term Frequency-Inverse Document Frequency) hesaplama\n",
    "def calculateIDF(doc):\n",
    "  idfDict = {}\n",
    "  len_doc = len(doc)\n",
    "\n",
    "  # tüm kelimeleri içeren ve başlangıçta değerleri 0 olarak ayarlanmış bir sözlük oluşturur. \n",
    "  # Bu sözlükte her kelime, anahtar (key) olarak saklanır ve karşılığı olan değer (value) ise başlangıçta 0 olarak belirlenir\n",
    "  idfDict = dict.fromkeys(doc[0].keys(), 0)\n",
    "  for word, val in idfDict.items():\n",
    "      idfDict[word] = math.log10(len_doc / (float(val) + 1))  #Belirli bir kelimenin kaç belgede geçtiğini kontrol eder ve bunu tüm belgelerin sayısına böler. \n",
    "        \n",
    "  return(idfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# belirli bir kelimenin bir belgede ne kadar önemli olduğunu gösterir\n",
    "idfs = calculateIDF([wordDict1, wordDict2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career</th>\n",
       "      <th>in</th>\n",
       "      <th>world</th>\n",
       "      <th>the</th>\n",
       "      <th>a</th>\n",
       "      <th>of</th>\n",
       "      <th>is</th>\n",
       "      <th>Data</th>\n",
       "      <th>current</th>\n",
       "      <th>learning</th>\n",
       "      <th>subset</th>\n",
       "      <th>Science</th>\n",
       "      <th>Amazing</th>\n",
       "      <th>machine</th>\n",
       "      <th>Deep</th>\n",
       "      <th>an</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    career       in    world      the        a       of       is     Data  \\\n",
       "0  0.10000  0.10000  0.10000  0.10000  0.00000  0.00000  0.10000  0.10000   \n",
       "1  0.00000  0.00000  0.00000  0.00000  0.12500  0.12500  0.12500  0.00000   \n",
       "2  0.30103  0.30103  0.30103  0.30103  0.30103  0.30103  0.30103  0.30103   \n",
       "\n",
       "   current  learning   subset  Science  Amazing  machine     Deep       an  \n",
       "0  0.10000   0.00000  0.00000  0.10000  0.10000  0.00000  0.00000  0.10000  \n",
       "1  0.00000   0.25000  0.12500  0.00000  0.00000  0.12500  0.12500  0.00000  \n",
       "2  0.30103   0.30103  0.30103  0.30103  0.30103  0.30103  0.30103  0.30103  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame([tf1,tf2,idfs])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8\"> \n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "### **TF-IDF (Term Frequency-Inverse Document Frequency) hesaplama**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career</th>\n",
       "      <th>in</th>\n",
       "      <th>world</th>\n",
       "      <th>the</th>\n",
       "      <th>a</th>\n",
       "      <th>of</th>\n",
       "      <th>is</th>\n",
       "      <th>Data</th>\n",
       "      <th>current</th>\n",
       "      <th>learning</th>\n",
       "      <th>subset</th>\n",
       "      <th>Science</th>\n",
       "      <th>Amazing</th>\n",
       "      <th>machine</th>\n",
       "      <th>Deep</th>\n",
       "      <th>an</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     career        in     world       the         a        of        is  \\\n",
       "0  0.030103  0.030103  0.030103  0.030103  0.000000  0.000000  0.030103   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.037629  0.037629  0.037629   \n",
       "\n",
       "       Data   current  learning    subset   Science   Amazing   machine  \\\n",
       "0  0.030103  0.030103  0.000000  0.000000  0.030103  0.030103  0.000000   \n",
       "1  0.000000  0.000000  0.075257  0.037629  0.000000  0.000000  0.037629   \n",
       "\n",
       "       Deep        an  \n",
       "0  0.000000  0.030103  \n",
       "1  0.037629  0.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "idf1 = computeTFIDF(tf1, idfs)\n",
    "idf2 = computeTFIDF(tf2, idfs)\n",
    "\n",
    "idf= pd.DataFrame([idf1, idf2])\n",
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#a95ec5\"> \n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "### **Sklearn Library**\n",
    "</span>\n",
    "\n",
    "TfidfVectorizer sınıfı verilen metinlerin TF-IDF skorlarını hesaplamak için kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32433627, 0.32433627, 0.32433627, 0.32433627, 0.32433627,\n",
       "        0.        , 0.32433627, 0.23076793, 0.        , 0.        ,\n",
       "        0.        , 0.32433627, 0.        , 0.32433627, 0.32433627],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34287126, 0.        , 0.24395573, 0.68574252, 0.34287126,\n",
       "        0.34287126, 0.        , 0.34287126, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "first_sent = \"Data Science is an Amazing career in the current world\"\n",
    "second_sent = \"Deep learning is a subset of machine learning\"\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "result = vec.fit_transform([first_sent,second_sent])\n",
    "result.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>an</th>\n",
       "      <th>career</th>\n",
       "      <th>current</th>\n",
       "      <th>data</th>\n",
       "      <th>deep</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>of</th>\n",
       "      <th>science</th>\n",
       "      <th>subset</th>\n",
       "      <th>the</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.230768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243956</td>\n",
       "      <td>0.685743</td>\n",
       "      <td>0.342871</td>\n",
       "      <td>0.342871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    amazing        an    career   current      data      deep        in  \\\n",
       "0  0.324336  0.324336  0.324336  0.324336  0.324336  0.000000  0.324336   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.342871  0.000000   \n",
       "\n",
       "         is  learning   machine        of   science    subset       the  \\\n",
       "0  0.230768  0.000000  0.000000  0.000000  0.324336  0.000000  0.324336   \n",
       "1  0.243956  0.685743  0.342871  0.342871  0.000000  0.342871  0.000000   \n",
       "\n",
       "      world  \n",
       "0  0.324336  \n",
       "1  0.000000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result.toarray(),columns= vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#c9486f\">\n",
    "\n",
    "--------------------------------------------------\n",
    " # 4. Word Embeddings \n",
    "  </span>\n",
    "\n",
    "  **Word Embeddings** (Kelime Gömmeleri), kelimeleri sayısal vektörler olarak temsil etmenin bir yoludur. Bu yaklaşım, benzer anlamlara sahip kelimelerin sayısal olarak benzer vektörlerle temsil edilmesini sağlar. Kelime gömmeleri, kelimelerin anlamlarını ve bağlamlarını düşük boyutlu bir uzayda modelleyerek, kelimeler arasındaki anlamsal ilişkileri yakalamaya olanak tanır.\n",
    "\n",
    "Özetle:\n",
    "- Kelimeleri sayısal vektörlerle temsil eder.\n",
    "- Benzer anlamdaki kelimeleri benzer vektörlerle gösterir.\n",
    "- Kelimeleri düşük boyutlu bir uzayda modelleyerek anlamsal ilişkileri yakalar.\n",
    "\n",
    "Bu tür kelime temsilleri, metin verisi ile çalışan makine öğrenimi ve doğal dil işleme projelerinde yaygın olarak kullanılır. Örneğin, **Flair**, **fastText**, **SpaCy** gibi önceden eğitilmiş modeller, word embeddings kullanarak metinleri anlamlandırma ve işlemeye yardımcı olur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2c82c8\">\n",
    "\n",
    "--------------------------------------------------\n",
    " ## **Kelime Vectörü - Word2Vec**\n",
    "  </span>\n",
    "\n",
    "\n",
    "\n",
    "Kelime vektörü, doğal dil işleme (NLP) alanında kullanılan bir tekniktir. Kelimeleri sayısal (numerik) vektörler olarak temsil eder. Bu vektörler, kelimelerin anlamlarını matematiksel bir biçimde ifade etmek amacıyla yüksek boyutlu uzayda (genellikle 100-300 boyutlu) yer alır. Kelimeler arasındaki anlam ilişkileri, bu vektörlerin uzayda nasıl yerleştirildiğiyle ilgilidir. Örneğin, anlamca birbirine yakın kelimeler (örneğin \"kral\" ve \"kraliçe\") uzayda birbirine daha yakın vektörlerle temsil edilir.\n",
    "\n",
    "**Kelime Vektörü Ne İçin Kullanılır?**\n",
    "\n",
    "Kelime vektörleri, birçok NLP görevinde kullanılır. Bunlardan bazıları şunlardır:\n",
    "\n",
    "- **Kelime Benzerliği ve Anlam Analizi:** İki kelimenin benzer olup olmadığını ölçmek için kullanılır. Örneğin, \"kral\" ve \"kraliçe\" kelimeleri arasındaki ilişki, bu kelimelerin vektörlerinin birbirine yakınlığı ile ölçülebilir.\n",
    "\n",
    "- **Anlamlı Sözcük Grupları Bulma:** Aynı kategoriye ait kelimeleri gruplayabilir. Örneğin, \"elma\", \"armut\", \"muz\" gibi kelimeler, meyve kategorisinde yer alır ve vektörleri birbirine yakın olabilir.\n",
    "\n",
    "- **Makine Çevirisi:** Diller arası çevirilerde kelime anlamlarını daha iyi yakalayabilmek için kullanılır.\n",
    "\n",
    "- **Metin Sınıflandırma:** Belgelerin sınıflandırılmasında kullanılabilir. Bir belgenin sınıfı, o belgede yer alan kelimelerin vektörlerinden elde edilen ortalama vektör ile belirlenebilir.\n",
    "\n",
    "- **Kelime Anlamı ile İlgili Sorunların Çözümü:** Özellikle homonimler (aynı yazılışa sahip farklı anlamlı kelimeler) için, kelime vektörleri anlam farklılıklarını belirlemek için kullanılabilir.\n",
    "\n",
    "\n",
    "\n",
    "**Kelime Vektörleri Nasıl Kullanılır?**\n",
    "- **Benzer Kelimeleri Bulma:** Bir kelimenin vektörü, diğer kelimelerin vektörleriyle karşılaştırılarak en benzer kelimeler bulunabilir.\n",
    "\n",
    "- **Matematiksel Operasyonlar:** Kelime vektörleri üzerinde toplama ve çıkarma işlemleri yapılarak, anlamlı sonuçlar elde edilebilir. Örneğin, \"kral\" - \"adam\" + \"kadın\" işlemi, \"kraliçe\" kelimesine yakın bir vektör üretir.\n",
    "\n",
    "\n",
    "Word2Vec, kelimeleri sayısal vektörler olarak temsil etmek için kullanılan popüler bir derin öğrenme modelidir. 2013 yılında Google tarafından geliştirilen bu model, kelimeler arasındaki anlamsal ilişkileri öğrenmek ve bu ilişkileri düşük boyutlu vektörler halinde temsil etmek için tasarlanmıştır.\n",
    "\n",
    "**Word2Vec Modelinin Özellikleri:**\n",
    "- `Dağıtımsal Hipotez:` Model, \"bir kelimenin anlamı, çevresindeki kelimelerle belirlenir\" ilkesine dayanır. Yani, bir kelimenin anlamını öğrenmek için o kelimenin diğer kelimelerle birlikte nasıl kullanıldığını gözlemler.\n",
    "\n",
    "**İki Temel Varyantı Vardır:**\n",
    "- **CBOW (Continuous Bag of Words):** Bu varyant, hedef kelimenin etrafındaki bağlam kelimelerini kullanarak o kelimeyi tahmin eder. Daha hızlı bir modeldir ve genellikle küçük veri setlerinde iyi sonuç verir.\n",
    "- **Skip-gram:** Bu varyant, hedef kelimeyi kullanarak çevresindeki bağlam kelimelerini tahmin eder. Büyük veri setlerinde daha iyi performans gösterir ve kelimeler arasındaki ince anlamsal farkları daha iyi yakalar.\n",
    "\n",
    "**Word2Vec'in Çalışma Prensibi:**\n",
    "- Word2Vec, bir kelimenin sayısal temsilini (vektör) öğrenmek için büyük miktarda metin verisi kullanır. Model, kelimeleri belirli bir boyutta (örneğin 100, 200, 300 boyutlu) vektörler olarak temsil eder.\n",
    "- Kelimeler arasındaki anlamsal yakınlık, bu vektörlerin birbirine yakınlığı ile belirlenir. Örneğin, \"kral\" ve \"kraliçe\" gibi kelimeler, Word2Vec modeli tarafından benzer vektörler olarak temsil edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: Levenshtein==0.25.1 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-Levenshtein) (0.25.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.9.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Computer' kelimesinin vektörü: [ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n",
      "Computer kelimesine en yakın kelimeler:\n",
      "computers: 0.80\n",
      "laptop: 0.66\n",
      "laptop_computer: 0.65\n",
      "Computer: 0.65\n",
      "com_puter: 0.61\n",
      "technician_Leonard_Luchko: 0.57\n",
      "mainframes_minicomputers: 0.56\n",
      "laptop_computers: 0.56\n",
      "PC: 0.55\n",
      "maker_Dell_DELL.O: 0.55\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Google News Word2Vec modelini indir ve yükle\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Kelime vektörlerini inceleme\n",
    "word_vector = model['computer']\n",
    "\n",
    "# kelimenin yüksek boyutlu uzayda temsilini oluşturan 300 adet kayan nokta sayısını (float) ekrana yazdırır\n",
    "print(f\"'Computer' kelimesinin vektörü: {word_vector}\")\n",
    "\n",
    "# Benzer kelimeleri bulma\n",
    "# Benzerlik, kosinüs benzerliği (cosine similarity) temelinde hesaplanır. \n",
    "# Kod, en benzer kelimelerin bir listesini ve bu kelimelerin benzerlik puanlarını ekrana yazdırır.\n",
    "similar_words = model.most_similar('computer')\n",
    "print(\"Computer kelimesine en yakın kelimeler:\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path,\n",
    "                                                         binary=True) # modeli yükleme ve binary=True ile modelin ikili dosya olduğunu belirtme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', 'in', 'for', 'that', 'is']\n"
     ]
    }
   ],
   "source": [
    "# Kelime dağarcığını almak için `index_to_key` kullanıyoruz\n",
    "vocab = list(model.index_to_key)\n",
    "\n",
    "# İlk 5 kelimeyi yazdırma\n",
    "print(vocab[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rastgele seçilen kelime: olfactory_ensheathing_cells\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "# Kelime dağarcığından rastgele bir kelime seçme\n",
    "random_word = choice(vocab)\n",
    "print(f\"Rastgele seçilen kelime: {random_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'olfactory_ensheathing_cells' kelimesinin vektörü: [-0.0625      0.03979492  0.00982666 -0.06787109 -0.08740234  0.0057373\n",
      " -0.00631714 -0.02563477 -0.01135254 -0.00686646]\n"
     ]
    }
   ],
   "source": [
    "word_vector = model[random_word]    \n",
    "print(f\"'{random_word}' kelimesinin vektörü: {word_vector[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varian_Semi\n",
      "Vaclav_Havel\n",
      "Amy_Hollstein_Pembroke\n",
      "ICICI_Pru_Life\n",
      "cotton_grower\n",
      "contributor_Selena_Maranjian\n",
      "Stenbit\n",
      "Briese\n",
      "STRAY_ROD_blared\n",
      "ankle_niggle\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(choice(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    word Present\n",
      "                                   ----- -----\n",
      "                                Narendra True\n",
      "                                   trump True\n",
      "                                    data True\n",
      "                                 machine True\n",
      "                                     nlp False\n"
     ]
    }
   ],
   "source": [
    "def check_vocab(model, check_words):\n",
    "    print(\"%40s %s\" % (\"word\", \"Present\"))\n",
    "    print(\"%40s %s\" % (\"-\"*5, \"-----\"))\n",
    "\n",
    "    for word in check_words:\n",
    "        print(\"%40s %s\" % (word, word in model.key_to_index))\n",
    "\n",
    "check_vocab(model, ['Narendra', 'trump', 'data', 'machine', 'nlp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    word Present\n",
      "                                   ----- -----\n",
      "                           Narendra_modi False\n",
      "                            donald_trump True\n",
      "                           united_states False\n",
      "                          united_kingdom False\n"
     ]
    }
   ],
   "source": [
    "check_vocab(model, ['Narendra_modi','donald_trump','united_states','united_kingdom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    word Present\n",
      "                                   ----- -----\n",
      "                           Narendra_Modi True\n",
      "                            Donald_Trump True\n",
      "                           United_States True\n",
      "                          United_Kingdom True\n"
     ]
    }
   ],
   "source": [
    "check_vocab(model, ['Narendra_Modi','Donald_Trump','United_States','United_Kingdom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\iremg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iremg\\AppData\\Local\\Temp\\ipykernel_14976\\1098050927.py:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  vector = model.word_vec(\"Narendra_Modi\")\n"
     ]
    }
   ],
   "source": [
    "vector = model.word_vec(\"Narendra_Modi\")\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.42187500e-01,  2.28515625e-01,  3.43750000e-01,  5.31250000e-01,\n",
       "       -3.41796875e-01, -3.71093750e-01, -1.72851562e-01,  1.43554688e-01,\n",
       "        1.07421875e-01,  4.95605469e-02,  2.02636719e-02, -1.25976562e-01,\n",
       "       -3.08593750e-01, -9.46044922e-03, -1.79687500e-01,  2.55859375e-01,\n",
       "       -6.54296875e-02, -2.53906250e-01, -4.88281250e-02,  8.59375000e-02,\n",
       "        3.82812500e-01,  1.40625000e-01,  1.35742188e-01, -6.00585938e-02,\n",
       "       -1.85546875e-01,  1.56250000e-01, -9.57031250e-02, -2.89062500e-01,\n",
       "        5.15625000e-01,  3.18908691e-03, -2.50000000e-01, -1.73828125e-01,\n",
       "       -3.30078125e-01,  5.35156250e-01, -2.49023438e-01,  3.47656250e-01,\n",
       "       -2.04101562e-01, -2.04101562e-01,  3.76953125e-01, -5.68847656e-02,\n",
       "        4.25781250e-01, -6.25000000e-02,  3.08593750e-01,  4.49218750e-01,\n",
       "        2.37304688e-01, -4.60937500e-01, -1.67968750e-01,  1.08886719e-01,\n",
       "       -6.65283203e-03,  1.37695312e-01, -6.88476562e-02,  1.31835938e-01,\n",
       "        2.96875000e-01, -3.68652344e-02, -5.12695312e-02, -4.73632812e-02,\n",
       "       -1.41601562e-01, -2.41210938e-01, -4.63867188e-02, -1.97753906e-02,\n",
       "        6.88476562e-02,  3.20312500e-01, -9.08203125e-02, -6.00585938e-02,\n",
       "        8.34960938e-02,  8.78906250e-02, -1.50390625e-01,  1.34765625e-01,\n",
       "        1.42578125e-01, -1.24511719e-02, -3.61328125e-01,  2.85156250e-01,\n",
       "        1.77734375e-01, -2.18200684e-03,  4.22363281e-02, -3.33984375e-01,\n",
       "        9.61914062e-02, -1.33666992e-02,  6.33239746e-04, -9.09423828e-03,\n",
       "       -1.14257812e-01,  1.95312500e-01, -6.93359375e-02, -2.83203125e-01,\n",
       "       -5.12695312e-02,  1.85394287e-03,  2.51953125e-01,  2.88085938e-02,\n",
       "       -5.83496094e-02, -2.61718750e-01, -3.20312500e-01, -3.05175781e-04,\n",
       "       -4.45312500e-01,  4.56542969e-02, -1.30859375e-01, -2.48046875e-01,\n",
       "        1.61132812e-01,  1.49414062e-01,  4.16015625e-01,  1.34765625e-01,\n",
       "       -3.88671875e-01,  2.38281250e-01,  3.88671875e-01, -1.57226562e-01,\n",
       "        3.49426270e-03,  3.61328125e-02, -1.83593750e-01,  3.22265625e-01,\n",
       "       -3.19824219e-02, -2.71484375e-01, -1.70898438e-01, -3.28125000e-01,\n",
       "        3.92578125e-01,  6.68945312e-02,  3.67187500e-01,  1.11816406e-01,\n",
       "       -4.32128906e-02, -1.36718750e-01,  1.54296875e-01,  1.08886719e-01,\n",
       "       -4.00390625e-02,  5.46875000e-02, -5.00000000e-01,  1.75781250e-01,\n",
       "       -1.51367188e-01,  5.15625000e-01,  2.07519531e-02,  1.35742188e-01,\n",
       "       -3.39355469e-02, -1.60156250e-01, -2.25585938e-01,  1.95312500e-01,\n",
       "        8.59375000e-02, -1.70898438e-01, -1.31835938e-01, -2.24609375e-01,\n",
       "        2.58789062e-02,  1.58203125e-01,  2.03125000e-01,  2.91015625e-01,\n",
       "        3.22265625e-01,  5.07812500e-02, -1.05957031e-01, -4.39453125e-01,\n",
       "        8.64257812e-02, -2.98828125e-01,  2.98828125e-01,  5.44433594e-02,\n",
       "        2.29492188e-01,  1.82617188e-01,  5.00000000e-01, -2.61718750e-01,\n",
       "        1.57226562e-01, -1.29882812e-01, -4.25781250e-01, -2.81250000e-01,\n",
       "       -3.02734375e-01,  1.01074219e-01,  2.03125000e-01,  1.28906250e-01,\n",
       "        7.51953125e-02, -5.07812500e-01, -4.60815430e-03, -2.75390625e-01,\n",
       "        2.75390625e-01, -2.89062500e-01, -4.78515625e-02, -2.10937500e-01,\n",
       "        8.05664062e-02,  2.38281250e-01, -2.96875000e-01, -6.95312500e-01,\n",
       "       -5.00000000e-01,  3.29589844e-02, -1.34277344e-02, -5.66406250e-01,\n",
       "       -6.07910156e-02, -3.37890625e-01, -3.33984375e-01, -3.96484375e-01,\n",
       "        4.22363281e-02,  4.57031250e-01, -4.17968750e-01, -4.00390625e-01,\n",
       "        1.99218750e-01,  1.00585938e-01,  1.25000000e-01,  6.73828125e-02,\n",
       "        1.15234375e-01, -2.01416016e-02, -2.53906250e-01,  1.62109375e-01,\n",
       "       -3.47656250e-01, -1.20117188e-01,  3.75000000e-01, -2.85156250e-01,\n",
       "       -5.20019531e-02,  7.22656250e-02,  7.86132812e-02, -2.94921875e-01,\n",
       "       -3.43750000e-01, -1.64062500e-01, -7.61718750e-02, -4.10156250e-01,\n",
       "        1.73828125e-01, -7.08007812e-02,  1.71875000e-01, -3.80859375e-01,\n",
       "        2.01416016e-02, -2.22656250e-01, -3.28125000e-01,  7.69042969e-03,\n",
       "        1.97265625e-01, -4.33593750e-01,  4.19921875e-02,  1.79687500e-01,\n",
       "       -1.64794922e-03,  4.58984375e-01, -2.96875000e-01, -2.98828125e-01,\n",
       "       -1.49414062e-01,  2.30468750e-01, -1.35742188e-01,  2.14843750e-01,\n",
       "       -1.50390625e-01,  1.31835938e-01,  1.58203125e-01,  8.30078125e-02,\n",
       "        5.62500000e-01,  8.15429688e-02, -2.24609375e-01,  6.25000000e-02,\n",
       "        2.41210938e-01, -1.79687500e-01,  1.79687500e-01,  1.31835938e-01,\n",
       "        3.14453125e-01, -1.82617188e-01, -1.03515625e-01,  3.68652344e-02,\n",
       "        5.62500000e-01, -1.08398438e-01,  2.63671875e-01, -1.51367188e-01,\n",
       "       -4.33593750e-01, -3.53515625e-01, -1.61132812e-02,  1.22558594e-01,\n",
       "        4.73632812e-02, -6.59179688e-02, -1.00097656e-01, -1.76757812e-01,\n",
       "        1.22070312e-01,  3.69140625e-01,  4.19921875e-01,  1.99218750e-01,\n",
       "        2.71484375e-01,  1.87500000e-01, -4.10156250e-02, -3.67187500e-01,\n",
       "       -4.95605469e-02,  4.08203125e-01,  3.14941406e-02, -1.20117188e-01,\n",
       "        4.37011719e-02,  9.08203125e-02,  4.66308594e-02,  2.92968750e-03,\n",
       "        6.29882812e-02, -2.59765625e-01, -2.96875000e-01,  3.58886719e-02,\n",
       "        6.34765625e-02, -3.41796875e-02,  1.88476562e-01, -1.72851562e-01,\n",
       "        1.91406250e-01,  4.15039062e-02,  2.18750000e-01,  1.81884766e-02,\n",
       "        1.09863281e-01, -2.94189453e-02, -5.71289062e-02,  1.98242188e-01,\n",
       "       -1.80664062e-01,  1.97753906e-02, -1.61132812e-01, -3.68652344e-02,\n",
       "       -3.46679688e-02, -1.15722656e-01, -7.08007812e-02,  7.81250000e-02,\n",
       "       -2.12890625e-01,  5.71289062e-02,  3.14941406e-02,  1.95312500e-01,\n",
       "        2.53906250e-01, -1.71875000e-01, -2.64892578e-02,  2.06054688e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narendra_Modi vektörünün büyüklüğü: 4.041469573974609\n"
     ]
    }
   ],
   "source": [
    "print(f\"Narendra_Modi vektörünün büyüklüğü: {np.linalg.norm(vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iremg\\AppData\\Local\\Temp\\ipykernel_14976\\2390323353.py:1: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(vector, kde=False, rug=True) # kde=False, çizgiyi kaldırır, rug=True, verilerin dağılımını gösterir\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(vector, kde=False, rug=True) # kde=False, çizgiyi kaldırır, rug=True, verilerin dağılımını gösterir\n",
    "plt.title(\"Özellik vektörünün histogramı\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#808000\">\n",
    "\n",
    "--------------------------------------------------\n",
    " ## **2 Kelime Arasındaki Benzerlik**\n",
    "  </span>\n",
    "\n",
    "  Eğer iki kelime arasındaki kosinüs benzerliği artarsa, bu kelimelerin anlamlarının daha yakın olduğu anlamına gelir. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arasındaki kosinüs benzerliği bread & jam is 0.22019492089748383\n"
     ]
    }
   ],
   "source": [
    "word1 = 'bread'\n",
    "word2 = 'jam'\n",
    "score = model.similarity(word1,word2)\n",
    "print(f\"arasındaki kosinüs benzerliği {word1} & {word2} is {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arasındaki kosinüs benzerliği bread & butter is 0.6417260766029358\n"
     ]
    }
   ],
   "source": [
    "word1 = 'bread'\n",
    "word2 = 'butter'\n",
    "score = model.similarity(word1,word2)\n",
    "print(f\"arasındaki kosinüs benzerliği {word1} & {word2} is {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arasındaki kosinüs benzerliği cycle & bicycle is 0.16222114861011505\n"
     ]
    }
   ],
   "source": [
    "word1 = 'cycle'\n",
    "word2 = 'bicycle'\n",
    "score = model.similarity(word1,word2)\n",
    "print(f\"arasındaki kosinüs benzerliği {word1} & {word2} is {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arasındaki kosinüs benzerliği computer & pc is 0.2656373977661133\n"
     ]
    }
   ],
   "source": [
    "word1 = 'computer'\n",
    "word2 = 'pc'\n",
    "score = model.similarity(word1,word2)\n",
    "print(f\"arasındaki kosinüs benzerliği {word1} & {word2} is {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arasındaki kosinüs benzerliği amazing & incredible is 0.9054000377655029\n"
     ]
    }
   ],
   "source": [
    "word1 = 'amazing'\n",
    "word2 = 'incredible'\n",
    "score = model.similarity(word1,word2)\n",
    "print(f\"arasındaki kosinüs benzerliği {word1} & {word2} is {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'computer' kelimesine en yakın kelimeler: [('computers', 0.7979379892349243), ('laptop', 0.6640493273735046), ('laptop_computer', 0.6548868417739868), ('Computer', 0.647333562374115), ('com_puter', 0.6082080006599426)]\n"
     ]
    }
   ],
   "source": [
    "# most similar words\n",
    "\n",
    "word = 'computer'\n",
    "result = model.most_similar(positive=[word], topn=5)\n",
    "print(f\"'{word}' kelimesine en yakın kelimeler: {result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
